{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake fundamentals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was built to be run on the following docker image: `jupyter/pyspark-notebook:spark-3.3.1`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: pyspark<3.4.0,>=3.3.0 in /usr/local/spark-3.3.1-bin-hadoop3/python (from delta-spark) (3.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.11.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark<3.4.0,>=3.3.0->delta-spark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    .appName(\"DeltaLakeFundamentals\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(spark).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake fundamentals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField('id', StringType(), True), \n",
    "        StructField('data_inversa', StringType(), True), \n",
    "        StructField('dia_semana', StringType(), True), \n",
    "        StructField('horario', StringType(), True), \n",
    "        StructField('uf', StringType(), True), \n",
    "        StructField('br', StringType(), True), \n",
    "        StructField('km', StringType(), True), \n",
    "        StructField('municipio', StringType(), True), \n",
    "        StructField('causa_acidente', StringType(), True), \n",
    "        StructField('tipo_acidente', StringType(), True), \n",
    "        StructField('classificacao_acidente', StringType(), True), \n",
    "        StructField('fase_dia', StringType(), True), \n",
    "        StructField('sentido_via', StringType(), True), \n",
    "        StructField('condicao_metereologica', StringType(), True), \n",
    "        StructField('tipo_pista', StringType(), True), \n",
    "        StructField('tracado_via', StringType(), True), \n",
    "        StructField('uso_solo', StringType(), True), \n",
    "        StructField('pessoas', IntegerType(), True), \n",
    "        StructField('mortos', IntegerType(), True), \n",
    "        StructField('feridos_leves', IntegerType(), True), \n",
    "        StructField('feridos_graves', IntegerType(), True), \n",
    "        StructField('ilesos', IntegerType(), True), \n",
    "        StructField('ignorados', IntegerType(), True), \n",
    "        StructField('feridos', IntegerType(), True), \n",
    "        StructField('veiculos', StringType(), True), \n",
    "        StructField('latitude', DoubleType(), True), \n",
    "        StructField('longitude', DoubleType(), True), \n",
    "        StructField('regional', StringType(), True), \n",
    "        StructField('delegacia', StringType(), True), \n",
    "        StructField('uop', StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "|    id|data_inversa|  dia_semana| horario| uf| br|   km|           municipio|      causa_acidente|       tipo_acidente|classificacao_acidente| fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|    latitude|   longitude|regional|delegacia|           uop|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "|260068|  2020-01-01|quarta-feira|05:40:00| PA|316|   84|SAO FRANCISCO DO ...|Falta de Atenção ...|Saída de leito ca...|   Com Vítimas Feridas|Pleno dia|Decrescente|             Céu Claro|   Simples|       Reta|     Não|      2|     0|            2|             0|     0|        0|      2|       1|  -1.3101929|-47.74456398| SPRF-PA| DEL01-PA|UOP02-DEL01-PA|\n",
      "|260073|  2020-01-01|quarta-feira|06:00:00| MG|262|  804|             UBERABA|Falta de Atenção ...| Colisão transversal|   Com Vítimas Feridas|Pleno dia|Decrescente|             Céu Claro|     Dupla|       Reta|     Sim|      4|     0|            1|             0|     3|        0|      1|       2|-19.76747537|-47.98725511| SPRF-MG| DEL13-MG|UOP01-DEL13-MG|\n",
      "|260087|  2020-01-01|quarta-feira|06:00:00| BA|116|  191|             CANUDOS|   Condutor Dormindo|Saída de leito ca...|    Com Vítimas Fatais|Pleno dia|  Crescente|               Nublado|   Simples|       Reta|     Não|      1|     1|            0|             0|     0|        0|      0|       1|-10.32002103|-39.06425211| SPRF-BA| DEL07-BA|UOP02-DEL07-BA|\n",
      "|260116|  2020-01-01|quarta-feira|10:08:00| SP|116|   71|           APARECIDA|Não guardar distâ...|    Colisão traseira|   Com Vítimas Feridas|Pleno dia|  Crescente|                   Sol|     Dupla|       Reta|     Sim|      3|     0|            2|             0|     1|        0|      2|       2|-22.85651665|-45.23114328| SPRF-SP| DEL08-SP|UOP01-DEL08-SP|\n",
      "|260129|  2020-01-01|quarta-feira|12:10:00| MG|262|380,9|             JUATUBA|   Condutor Dormindo|Saída de leito ca...|   Com Vítimas Feridas|Pleno dia|  Crescente|             Céu Claro|     Dupla|      Curva|     Não|      1|     0|            1|             0|     0|        0|      1|       1|  -19.947864|  -44.381226| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"/data/acidentes/datatran2020.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Delta Table is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"/data/delta/acidentes/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you are having trouble in writing the Delta Table, remember to check the permissions of the folder, including the newly created._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read from a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_delta = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(\"/data/delta/acidentes/\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can execute queries as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+--------+---+\n",
      "|    id|data_inversa|dia_semana| horario| uf|\n",
      "+------+------------+----------+--------+---+\n",
      "|263804|  2020-01-19|   domingo|12:30:00| SC|\n",
      "|263806|  2020-01-19|   domingo|14:50:00| SC|\n",
      "|263807|  2020-01-19|   domingo|14:00:00| RJ|\n",
      "|263809|  2020-01-19|   domingo|15:30:00| RS|\n",
      "|263812|  2020-01-19|   domingo|15:15:00| GO|\n",
      "+------+------------+----------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_delta.select([\"id\", \"data_inversa\", \"dia_semana\", \"horario\", \"uf\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add new data to the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new data is just a matter of appending the new data to the Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2019 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"/data/acidentes/datatran2019.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2019\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"/data/delta/acidentes/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows in the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. View the history (logs) of the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log of the Delta Table is a record of all the operations that have been performed on the table. It contains a detailed description of each operation performed, including all the metadata about the operation.\n",
    "\n",
    "To read the log, we can use a special python object called `DeltaTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2023-02-14 01:12:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 4, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-02-14 01:11:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 4, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/data/delta/acidentes/\")\n",
    "\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                   |\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|1      |2023-02-14 01:12:04.86 |WRITE    |{mode -> Append, partitionBy -> []}   |\n",
      "|0      |2023-02-14 01:11:28.625|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read a specific version of the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nothing is specified, the Spark will read the latest version of the Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131132"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(\"/data/delta/acidentes/\")\n",
    ")\n",
    "\n",
    "df_acidentes_latest.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's also possible to read a specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_version_0 = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", 0)\n",
    "    .load(\"/data/delta/acidentes/\")\n",
    ")\n",
    "\n",
    "df_acidentes_version_0.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the version 0 was specified, it only contains the data from the first operation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Revert to a previous version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another operation performed by the `DeltaTable` object. [Link](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToVersion(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the latest version doesn't contain the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RESTORE** operation is also stored in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                   |\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|2      |2023-02-14 01:33:27.373|RESTORE  |{version -> 0, timestamp -> null}     |\n",
      "|1      |2023-02-14 01:12:04.86 |WRITE    |{mode -> Append, partitionBy -> []}   |\n",
      "|0      |2023-02-14 01:11:28.625|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in practice, no information is lost.\n",
    "\n",
    "Let's restore back to the version with the data from 2020 and 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToVersion(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Update "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update operation is also done by the `DeltaTable` object. But we will use the `update` method with the SQL syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write the data from 2016 to the delta table. This data contains the \"data_inversa\" columns wrongly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_inversa|\n",
      "+------------+\n",
      "|    10/06/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2016 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"/data/acidentes/datatran2016.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes_2016.select(\"data_inversa\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2016\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"/data/delta/acidentes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227495"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    UPDATE acidentes_latest\n",
    "    SET data_inversa = CAST( TO_DATE(data_inversa, 'dd/MM/yy') AS STRING)\n",
    "    WHERE data_inversa LIKE '%/16'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"%/16\") ).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Merge "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merge operation, also known as upsert, is a mix of insert and update. It's also done by the `DeltaTable` object. But we will use the `merge` method with the SQL syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the utility of this method, we'll simulate a scenario where we have a data source that is constantly updating the data with new counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2018 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"/data/acidentes/datatran2018.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes_2018_zero = df_acidentes_2018.withColumn(\"pessoas\", F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2018_zero\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"/data/delta/acidentes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296827"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|data_inversa|pessoas|\n",
      "+------------+-------+\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"2018-%\") ).select([\"data_inversa\", \"pessoas\"]).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's merge the data with the correct counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "df_acidentes_2018.createOrReplaceTempView(\"acidentes_2018_new_counts\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    MERGE INTO acidentes_latest\n",
    "    USING acidentes_2018_new_counts\n",
    "\n",
    "    ON acidentes_latest.id = acidentes_2018_new_counts.id\n",
    "    AND acidentes_latest.data_inversa = acidentes_2018_new_counts.data_inversa\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET pessoas = acidentes_latest.pessoas + acidentes_2018_new_counts.pessoas\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT *\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"2018-%\") ).select([\"data_inversa\", \"pessoas\"]).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
