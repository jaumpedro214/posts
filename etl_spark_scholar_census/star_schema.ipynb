{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Star Schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the creation of a Star Schema from raw data using Apache Spark.\n",
    "\n",
    "The data used in this notebook is from the Brazilian Basic Education Census, which is available at the [Brazilian government's open data portal](https://download.inep.gov.br/dados_abertos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedro/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"CensoEscolarStarSchema\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming CSV to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation aims to increase the speed of data loading by using Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data\n",
    "data_csv = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"encoding\", \"latin1\")\n",
    "    .load(\"./data/*.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.write.parquet(\"./data/censo_escolar.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"parquet\")\n",
    "    .load(\"./data/censo_escolar.parquet/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions are...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the dimensions based on a configuration dict.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"DIM_NAME\":{\n",
    "        # The fields are the table columns\n",
    "        \"fields\":[\n",
    "            {\n",
    "                \"field\":\"FIELD_1_NAME\", # The column name\n",
    "                \"type\":\"FIELD_1_TYPE\",  # The column type in spark\n",
    "            },\n",
    "            {\n",
    "                \"field\":\"FIELD_2_NAME\",\n",
    "                \"type\":\"FIELD_2_TYPE\",\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER_DIMENSIONS = [\n",
    "    \"TP_DEPENDENCIA\",\n",
    "    \"TP_LOCALIZACAO\",\n",
    "    \"IN_AGUA_POTAVEL\",\n",
    "    \"IN_ENERGIA_INEXISTENTE\",\n",
    "    \"IN_ESGOTO_INEXISTENTE\",\n",
    "    \"IN_BANHEIRO\",\n",
    "    \"IN_BIBLIOTECA\",\n",
    "    \"IN_REFEITORIO\",\n",
    "    \"IN_COMPUTADOR\",\n",
    "    \"IN_INTERNET\",\n",
    "    \"IN_EQUIP_NENHUM\"\n",
    "]\n",
    "\n",
    "DIMENSION_TABLES_CONFIG = {\n",
    "    \"DIM_LOCAL\":{\n",
    "        \"fields\": [\n",
    "            {\"field\":\"NO_UF\", \"type\":\"string\",},\n",
    "            {\"field\":\"SG_UF\", \"type\":\"string\",},\n",
    "            {\"field\":\"CO_UF\", \"type\":\"string\",},\n",
    "            {\"field\":\"NO_MUNICIPIO\", \"type\":\"string\",},\n",
    "            {\"field\":\"CO_MUNICIPIO\", \"type\":\"string\",}\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "DIMENSION_TABLES_CONFIG.update(\n",
    "    {\n",
    "        \"DIM_\"+dimension.upper():{\n",
    "            \"fields\": [\n",
    "                {\"field\":dimension, \"type\":\"integer\"} \n",
    "            ]\n",
    "        }\n",
    "        for dimension in INTEGER_DIMENSIONS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dimensions table in Postgres\n",
    "-----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the properties of the Postgres Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER = \"censo\"\n",
    "POSTGRES_PASSWORD = \"123\"\n",
    "POSTGRES_DB = \"censo_escolar\"\n",
    "\n",
    "# Used to connect to the PostgreSQL database server\n",
    "# in spark session\n",
    "POSTGRES_CONFIG = {\n",
    "    \"url\":f\"jdbc:postgresql://localhost:5432/{POSTGRES_DB}\",\n",
    "    \"properties\":{\n",
    "        \"user\":POSTGRES_USER, \n",
    "        \"password\":POSTGRES_PASSWORD,\n",
    "        \"driver\":\"org.postgresql.Driver\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing connection to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "\n",
    "    dbname=POSTGRES_DB,\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a Dimension table in Postgres using the configuration in DIMENSION_TABLES_CONFIG and adding an id column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the dimensions\n",
    "Spark will create a table with the name of the dimension and the columns in the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-25 02:01:51.073109] Writing DIM_LOCAL\n",
      "[2022-07-25 02:02:00.865734] Wrote DIM_LOCAL\n",
      "[2022-07-25 02:02:00.889941] Added primary key to DIM_LOCAL\n",
      "[2022-07-25 02:02:00.890092] Done\n",
      "[2022-07-25 02:02:00.890113] Writing DIM_TP_DEPENDENCIA\n",
      "[2022-07-25 02:02:02.179984] Wrote DIM_TP_DEPENDENCIA\n",
      "[2022-07-25 02:02:02.188489] Added primary key to DIM_TP_DEPENDENCIA\n",
      "[2022-07-25 02:02:02.188640] Done\n",
      "[2022-07-25 02:02:02.188660] Writing DIM_TP_LOCALIZACAO\n",
      "[2022-07-25 02:02:03.262895] Wrote DIM_TP_LOCALIZACAO\n",
      "[2022-07-25 02:02:03.275347] Added primary key to DIM_TP_LOCALIZACAO\n",
      "[2022-07-25 02:02:03.276650] Done\n",
      "[2022-07-25 02:02:03.276778] Writing DIM_IN_AGUA_POTAVEL\n",
      "[2022-07-25 02:02:04.478306] Wrote DIM_IN_AGUA_POTAVEL\n",
      "[2022-07-25 02:02:04.492452] Added primary key to DIM_IN_AGUA_POTAVEL\n",
      "[2022-07-25 02:02:04.492676] Done\n",
      "[2022-07-25 02:02:04.492703] Writing DIM_IN_ENERGIA_INEXISTENTE\n",
      "[2022-07-25 02:02:05.313743] Wrote DIM_IN_ENERGIA_INEXISTENTE\n",
      "[2022-07-25 02:02:05.342052] Added primary key to DIM_IN_ENERGIA_INEXISTENTE\n",
      "[2022-07-25 02:02:05.342212] Done\n",
      "[2022-07-25 02:02:05.342233] Writing DIM_IN_ESGOTO_INEXISTENTE\n",
      "[2022-07-25 02:02:06.257627] Wrote DIM_IN_ESGOTO_INEXISTENTE\n",
      "[2022-07-25 02:02:06.275871] Added primary key to DIM_IN_ESGOTO_INEXISTENTE\n",
      "[2022-07-25 02:02:06.276108] Done\n",
      "[2022-07-25 02:02:06.276239] Writing DIM_IN_BANHEIRO\n",
      "[2022-07-25 02:02:07.192189] Wrote DIM_IN_BANHEIRO\n",
      "[2022-07-25 02:02:07.228156] Added primary key to DIM_IN_BANHEIRO\n",
      "[2022-07-25 02:02:07.228722] Done\n",
      "[2022-07-25 02:02:07.228749] Writing DIM_IN_BIBLIOTECA\n",
      "[2022-07-25 02:02:07.999468] Wrote DIM_IN_BIBLIOTECA\n",
      "[2022-07-25 02:02:08.008361] Added primary key to DIM_IN_BIBLIOTECA\n",
      "[2022-07-25 02:02:08.008531] Done\n",
      "[2022-07-25 02:02:08.008557] Writing DIM_IN_REFEITORIO\n",
      "[2022-07-25 02:02:08.810625] Wrote DIM_IN_REFEITORIO\n",
      "[2022-07-25 02:02:08.825193] Added primary key to DIM_IN_REFEITORIO\n",
      "[2022-07-25 02:02:08.825334] Done\n",
      "[2022-07-25 02:02:08.825360] Writing DIM_IN_COMPUTADOR\n",
      "[2022-07-25 02:02:09.528062] Wrote DIM_IN_COMPUTADOR\n",
      "[2022-07-25 02:02:09.561286] Added primary key to DIM_IN_COMPUTADOR\n",
      "[2022-07-25 02:02:09.561411] Done\n",
      "[2022-07-25 02:02:09.561431] Writing DIM_IN_INTERNET\n",
      "[2022-07-25 02:02:10.363343] Wrote DIM_IN_INTERNET\n",
      "[2022-07-25 02:02:10.375072] Added primary key to DIM_IN_INTERNET\n",
      "[2022-07-25 02:02:10.375296] Done\n",
      "[2022-07-25 02:02:10.375337] Writing DIM_IN_EQUIP_NENHUM\n",
      "[2022-07-25 02:02:11.188565] Wrote DIM_IN_EQUIP_NENHUM\n",
      "[2022-07-25 02:02:11.255191] Added primary key to DIM_IN_EQUIP_NENHUM\n",
      "[2022-07-25 02:02:11.255319] Done\n"
     ]
    }
   ],
   "source": [
    "# Write data to Postgres\n",
    "# Using the configuration in DIMENSION_TABLES_CONFIG\n",
    "# With id as the primary key\n",
    "\n",
    "for table_name, table_config in DIMENSION_TABLES_CONFIG.items():\n",
    "    \n",
    "    print(f\"[{datetime.now()}] Writing {table_name}\")\n",
    "    \n",
    "    data\\\n",
    "    .select(\n",
    "        [\n",
    "            F\n",
    "            .col(field[\"field\"])\n",
    "            .cast(field[\"type\"])\n",
    "            .alias(field[\"field\"])\n",
    "            \n",
    "            for field\n",
    "            in table_config[\"fields\"]\n",
    "        ]\n",
    "    )\\\n",
    "    .distinct()\\\n",
    "    .withColumn(\n",
    "        \"id\", F.monotonically_increasing_id()\n",
    "    )\\\n",
    "    .write\\\n",
    "    .jdbc(\n",
    "        **POSTGRES_CONFIG,\n",
    "        table=table_name,\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    \n",
    "    print(f\"[{datetime.now()}] Wrote {table_name}\")\n",
    "    # Define id as the primary key\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        f\"ALTER TABLE {table_name} ADD PRIMARY KEY (id);\"\n",
    "    )\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"[{datetime.now()}] Added primary key to {table_name}\")\n",
    "    print(f\"[{datetime.now()}] Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facts table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the facts table follows a different pattern than the dimensions.\n",
    "\n",
    "The table schema is previously defined to properly define the foreing keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Defining the facts table schema\n",
    "Metrics + Facts + Dimensions (Foreign Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACT_TABLE_NAME = \"FACT_CENSO_ESCOLAR\"\n",
    "\n",
    "FACT_COLUMNS = [\n",
    "    \"QT_DOC_BAS\",\t# Número de Docentes da Educação Básica\n",
    "    \"QT_DOC_INF\",\t# Número de Docentes da Educação Infantil\n",
    "    \"QT_DOC_FUND\",\t# Número de Docentes do Ensino Fundamental\n",
    "    \"QT_DOC_MED\",\t# Número de Docentes do Ensino Médio\n",
    "\n",
    "    \"QT_MAT_BAS\",\t# Número de Matrículas na Educação Básica (TOTAL)\n",
    "    \"QT_MAT_INF\",\t# Número de Matrículas na Educação Infantil\n",
    "    \"QT_MAT_FUND\",\t# Número de Matrículas no Ensino Fundamental\n",
    "    \"QT_MAT_MED\",\t# Número de Matrículas no Ensino Médio\n",
    "\n",
    "    \"QT_MAT_BAS_ND\",\t    # Número de Matrículas na Educação Básica - Cor/Raça Não Declarada\n",
    "    \"QT_MAT_BAS_BRANCA\",\t# Número de Matrículas na Educação Básica - Cor/Raça Branca\n",
    "    \"QT_MAT_BAS_PRETA\",\t    # Número de Matrículas na Educação Básica - Cor/Raça Preta\n",
    "    \"QT_MAT_BAS_PARDA\",\t    # Número de Matrículas na Educação Básica - Cor/Raça Parda\n",
    "    \"QT_MAT_BAS_AMARELA\",\t# Número de Matrículas na Educação Básica - Cor/Raça Amarela\n",
    "    \"QT_MAT_BAS_INDIGENA\",\t# Número de Matrículas na Educação Básica - Cor/Raça Indígena\n",
    "    \n",
    "    \"NU_ANO_CENSO\"\n",
    "]\n",
    "\n",
    "FACT_CONFIG = {\n",
    "    fact:{\n",
    "        \"fields\": [\n",
    "            {\"field\":fact, \"type\":\"integer\"}\n",
    "        ]\n",
    "    }\n",
    "    for fact in FACT_COLUMNS\n",
    "}\n",
    "\n",
    "DIMENSION_ID_CONFIG = {\n",
    "    table_name:[\n",
    "        field['field'] \n",
    "        for field \n",
    "        in table_fields['fields']\n",
    "    ]\n",
    "    for table_name, table_fields in DIMENSION_TABLES_CONFIG.items()\n",
    "}\n",
    "\n",
    "FACT_TABLE_ALL_COLUMNS_ORDERED = FACT_COLUMNS + list(map(lambda col:\"ID_\"+col, DIMENSION_ID_CONFIG.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inserting the data into the facts table, we need to create a function to create the facts table in Postgres\n",
    "\n",
    "The code below creates the facts table in Postgres using the configuration in FACT_TABLES_CONFIG and adding an id column for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "# Using the configuration in FACT_CONFIG\n",
    "# With id as the primary key\n",
    "\n",
    "# Avoid inserting a backslash into a f-string\n",
    "comma_break_line = \",\\n\\t\\t\\t\"\n",
    "facts_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {FACT_TABLE_NAME} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        { \n",
    "            comma_break_line.join(\n",
    "                [\n",
    "                    f\"{field} INTEGER\" \n",
    "                    for field in FACT_COLUMNS\n",
    "                ]\n",
    "                +[\n",
    "                    f\"ID_{dim_table} BIGINT\"\n",
    "                    for dim_table in DIMENSION_ID_CONFIG.keys()\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "    );\n",
    "    \n",
    "    -- Adding Foreign Keys\n",
    "    ALTER TABLE {FACT_TABLE_NAME}\n",
    "    {\n",
    "        comma_break_line.join(\n",
    "            [\n",
    "                f\"ADD CONSTRAINT {FACT_TABLE_NAME}_{dim_table}_fk FOREIGN KEY (ID_{dim_table}) REFERENCES {dim_table}(id)\"\n",
    "                for dim_table in DIMENSION_ID_CONFIG.keys()\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CREATE TABLE IF NOT EXISTS FACT_CENSO_ESCOLAR (\n",
      "        id SERIAL PRIMARY KEY,\n",
      "        QT_DOC_BAS INTEGER,\n",
      "\t\t\tQT_DOC_INF INTEGER,\n",
      "\t\t\tQT_DOC_FUND INTEGER,\n",
      "\t\t\tQT_DOC_MED INTEGER,\n",
      "\t\t\tQT_MAT_BAS INTEGER,\n",
      "\t\t\tQT_MAT_INF INTEGER,\n",
      "\t\t\tQT_MAT_FUND INTEGER,\n",
      "\t\t\tQT_MAT_MED INTEGER,\n",
      "\t\t\tQT_MAT_BAS_ND INTEGER,\n",
      "\t\t\tQT_MAT_BAS_BRANCA INTEGER,\n",
      "\t\t\tQT_MAT_BAS_PRETA INTEGER,\n",
      "\t\t\tQT_MAT_BAS_PARDA INTEGER,\n",
      "\t\t\tQT_MAT_BAS_AMARELA INTEGER,\n",
      "\t\t\tQT_MAT_BAS_INDIGENA INTEGER,\n",
      "\t\t\tNU_ANO_CENSO INTEGER,\n",
      "\t\t\tID_DIM_LOCAL BIGINT,\n",
      "\t\t\tID_DIM_TP_DEPENDENCIA BIGINT,\n",
      "\t\t\tID_DIM_TP_LOCALIZACAO BIGINT,\n",
      "\t\t\tID_DIM_IN_AGUA_POTAVEL BIGINT,\n",
      "\t\t\tID_DIM_IN_ENERGIA_INEXISTENTE BIGINT,\n",
      "\t\t\tID_DIM_IN_ESGOTO_INEXISTENTE BIGINT,\n",
      "\t\t\tID_DIM_IN_BANHEIRO BIGINT,\n",
      "\t\t\tID_DIM_IN_BIBLIOTECA BIGINT,\n",
      "\t\t\tID_DIM_IN_REFEITORIO BIGINT,\n",
      "\t\t\tID_DIM_IN_COMPUTADOR BIGINT,\n",
      "\t\t\tID_DIM_IN_INTERNET BIGINT,\n",
      "\t\t\tID_DIM_IN_EQUIP_NENHUM BIGINT\n",
      "    );\n",
      "    \n",
      "    -- Adding Foreign Keys\n",
      "    ALTER TABLE FACT_CENSO_ESCOLAR\n",
      "    ADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_LOCAL_fk FOREIGN KEY (ID_DIM_LOCAL) REFERENCES DIM_LOCAL(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_TP_DEPENDENCIA_fk FOREIGN KEY (ID_DIM_TP_DEPENDENCIA) REFERENCES DIM_TP_DEPENDENCIA(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_TP_LOCALIZACAO_fk FOREIGN KEY (ID_DIM_TP_LOCALIZACAO) REFERENCES DIM_TP_LOCALIZACAO(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_AGUA_POTAVEL_fk FOREIGN KEY (ID_DIM_IN_AGUA_POTAVEL) REFERENCES DIM_IN_AGUA_POTAVEL(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_ENERGIA_INEXISTENTE_fk FOREIGN KEY (ID_DIM_IN_ENERGIA_INEXISTENTE) REFERENCES DIM_IN_ENERGIA_INEXISTENTE(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_ESGOTO_INEXISTENTE_fk FOREIGN KEY (ID_DIM_IN_ESGOTO_INEXISTENTE) REFERENCES DIM_IN_ESGOTO_INEXISTENTE(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_BANHEIRO_fk FOREIGN KEY (ID_DIM_IN_BANHEIRO) REFERENCES DIM_IN_BANHEIRO(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_BIBLIOTECA_fk FOREIGN KEY (ID_DIM_IN_BIBLIOTECA) REFERENCES DIM_IN_BIBLIOTECA(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_REFEITORIO_fk FOREIGN KEY (ID_DIM_IN_REFEITORIO) REFERENCES DIM_IN_REFEITORIO(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_COMPUTADOR_fk FOREIGN KEY (ID_DIM_IN_COMPUTADOR) REFERENCES DIM_IN_COMPUTADOR(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_INTERNET_fk FOREIGN KEY (ID_DIM_IN_INTERNET) REFERENCES DIM_IN_INTERNET(id),\n",
      "\t\t\tADD CONSTRAINT FACT_CENSO_ESCOLAR_DIM_IN_EQUIP_NENHUM_fk FOREIGN KEY (ID_DIM_IN_EQUIP_NENHUM) REFERENCES DIM_IN_EQUIP_NENHUM(id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(facts_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the function to create the facts table in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-25 02:02:11.470506] Creating facts table\n",
      "[2022-07-25 02:02:11.493910] Created facts table\n",
      "[2022-07-25 02:02:11.494028] Done\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] Creating facts table\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(facts_table_sql)\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    conn.rollback()\n",
    "    cursor.close()\n",
    "else:\n",
    "    print(f\"[{datetime.now()}] Created facts table\")\n",
    "    print(f\"[{datetime.now()}] Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_data = data\\\n",
    "    .select(\n",
    "        [\n",
    "            *chain(\n",
    "                *DIMENSION_ID_CONFIG.values(), \n",
    "                FACT_CONFIG.keys()\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the ids for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the id of the dimensions\n",
    "\n",
    "for table_name, table_fields in DIMENSION_ID_CONFIG.items():\n",
    "    \n",
    "    # Read the dimension data from Postgres\n",
    "    dim_table = spark.read\\\n",
    "        .jdbc(\n",
    "            **POSTGRES_CONFIG,\n",
    "            table=table_name,\n",
    "        )\\\n",
    "        .withColumnRenamed(\"id\", f\"ID_{table_name}\")\n",
    "    \n",
    "    # Join the dimension data with the fact data\n",
    "    facts_data = facts_data\\\n",
    "        .join(\n",
    "            dim_table,\n",
    "            on=table_fields,\n",
    "            how=\"left\"\n",
    "        )\\\n",
    "        .drop(*table_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns to match the fact table on postgres\n",
    "# and save the data\n",
    "facts_data\\\n",
    "    .select(*FACT_TABLE_ALL_COLUMNS_ORDERED)\\\n",
    "    .write\\\n",
    "    .jdbc(\n",
    "        **POSTGRES_CONFIG,\n",
    "        table=FACT_TABLE_NAME,\n",
    "        mode=\"append\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9\n",
    "\n",
    "https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "\n",
    "https://www.psycopg.org/docs/usage.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
